---
title: "Main"
layout: default
permalink: /
---

### Publications

#### 2024
-----------

| [SILO Language Models: Isolating Legal Risk in a Nonparametric Datastore](https://arxiv.org/abs/2308.04430)<br><sub>Sewon Min<sup>\*</sup>, **Suchin Gururangan<sup>\*</sup>**, Eric Wallace, Hannaneh Hajishirzi, Noah A. Smith, Luke Zettlemoyer</sub><br><sub><sup>\*</sup>Equal Contribution</sub><br><sub>*ICLR 2024*</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/kernelmachine/silo-lm)</span></sub> |
| [AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters](https://arxiv.org/abs/2401.06408)<br><sub>Li Lucy, **Suchin Gururangan**, Luca Soldaini, Emma Strubell, David Bamman, Lauren Klein, Jesse Dodge</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/lucy3/whos_filtered)</span></sub> |

#### 2023
-----------

| [Time is Encoded in the Weights of Finetuned Language Models](https://arxiv.org/abs/2312.13401)<br><sub>Kai Nylund, **Suchin Gururangan**, Noah A. Smith</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/KaiNylund/lm-weights-encode-time)</span></sub> |
| [Scaling Expert Language Models with Unsupervised Domain Discovery](https://arxiv.org/abs/2303.14177)<br><sub>**Suchin Gururangan<sup>\*</sup>**, Margaret Li<sup>\*</sup>, Mike Lewis, Weijia Shi, Tim Althoff, Noah A. Smith, Luke Zettlemoyer</sub><br><sub><sup>\*</sup>Equal Contribution</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/kernelmachine/cbtm)</span></sub> |
| [Editing Models with Task Arithmetic](https://arxiv.org/abs/2212.04089)<br><sub>Gabriel Ilharco, Marco Tulio Riberio, Mitchell Wortsman, **Suchin Gururangan**, Ludwig Schmidt, Hannaneh Hajishirzi, Ali Farhadi</sub><br><sub>*ICLR 2023*</sub>| <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/mlfoundations/task_vectors)</span></sub> |

#### 2022
-----------

| [lo-fi: distributed fine-tuning without communication](https://arxiv.org/abs/2210.11948)<br><sub>Mitchell Wortsman, **Suchin Gururangan**, Shen Li, Ali Farhadi, Ludwig Schmidt, Michael Rabbat, Ari S. Morcos<br></sub><sub>*TMLR*</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/kernelmachine/lofi)</span></sub> |
| [M2D2: A Massively Multi-Domain Language Modeling Dataset](https://arxiv.org/abs/2210.07370)<br><sub>Machel Reid, Victor Zhong, **Suchin Gururangan**, Luke Zettlemoyer </sub> <br><sub>*EMNLP 2022*</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/machelreid/m2d2)</span></sub> |
| [Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection](https://arxiv.org/abs/2201.10474)<br><sub>**Suchin Gururangan**, Dallas Card, Sarah K. Dreier, Emily K. Gade, Leroy Wang, Blarry Wang,Luke Zettlemoyer, and Noah A. Smith</sub><br><sub>*EMNLP 2022*</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/kernelmachine/quality-filter)</span></sub> |
| [kNN-Prompt: Nearest Neighbor Zero-Shot Inference](https://arxiv.org/abs/2205.13792)<br><sub>Weijia Shi, Julian Michael, **Suchin Gururangan**,  and Luke Zettlemoyer</sub><br><sub>*EMNLP 2022*</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/swj0419/kNN_prompt)</span></sub> |
| [Branch-Train-Merge: Embarrassingly Parallel Training of Expert Language Models](https://arxiv.org/abs/2208.03306)<br><sub>Margaret Li<sup>\*</sup>, **Suchin Gururangan<sup>\*</sup>**, Tim Dettmers, Mike Lewis, Noah A. Smith, and Luke Zettlemoyer</sub><br><sub><sup>\*</sup>Equal Contribution</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/hadasah/btm)</span></sub> |
| [Time Waits for No One! Analysis and Challenges of Temporal Misalignment](https://arxiv.org/abs/2111.07408)<br><sub>Kelvin Luu, Daniel Khashabi, **Suchin Gururangan**, Karishma Mandyam, and Noah A. Smith</sub><br><sub>*NAACL 2022*</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/Kel-Lu/time-waits-for-no-one)</span></sub> |
| [DEMix Layers: Disentangling Domains for Modular Language Modeling](https://arxiv.org/abs/2108.05036)<br><sub>**Suchin Gururangan**, Mike Lewis, Ari Holtzman, Noah A. Smith, and Luke Zettlemoyer</sub><br><sub>*NAACL 2022*</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/kernelmachine/demix)</span></sub> |

####  2021
-----------

| [All That's 'Human' Is Not Gold: Evaluating Human Evaluation of Generated Text](https://arxiv.org/abs/2107.00061)<br><sub>Elizabeth Clark, Tal August, Sofia Serrano, Nikita Haduong, **Suchin Gururangan**, and Noah A. Smith</sub><br><sub>*ACL 2021*</sub><br>âœ¨<sub>**Outstanding Paper Award**</sub>âœ¨| |
| [Expected Validation Performance and Estimation of a Random Variable's Maximum](https://arxiv.org/abs/2110.00613)<br><sub>Jesse Dodge, **Suchin Gururangan**, Roy Schwartz, Dallas Card, and Noah A. Smith</sub> | |
| [Detoxifying Language Models Risks Marginalizing Minority Voices](https://arxiv.org/abs/2104.06390)<br><sub>Albert Xu, Eshaan Pathak, Eric Wallace, **Suchin Gururangan**, Maarten Sap, and Dan Klein</sub><br><sub>*NAACL 2021*</sub>| |

####  2020
-----------

| [RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models](https://arxiv.org/abs/2009.11462)<br><sub> Sam Gehman, **Suchin Gururangan**, Maarten Sap, Yejin Choi, and Noah A. Smith</sub><br><sub>*EMNLP Findings 2020*</sub>| <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/allenai/real-toxicity-prompts)</span></sub> |
| [Don't Stop Pretraining: Adapt Language Models to Domains and Tasks](https://arxiv.org/abs/2004.10964)<br><sub>**Suchin Gururangan**, Ana MarasoviÄ‡, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith </sub><br><sub>*ACL 2020*</sub><br>âœ¨<sub>**Honorable Mention for Best Overall Paper**</sub>âœ¨| <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/allenai/dont-stop-pretraining)</span></sub> |


#### 2019
-----------

| [Variational Pretraining for Semi-supervised Text Classification](https://arxiv.org/abs/1906.02242)<br><sub>**Suchin Gururangan**,Tam Dang, Dallas Card, and Noah A. Smith</sub><br><sub>*ACL 2019*</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/allenai/vampire)</span></sub> |
| [Show Your Work: Improved Reporting of Experimental Results](https://arxiv.org/abs/1909.03004)<br><sub>Jesse Dodge, **Suchin Gururangan**, Roy Schwartz, Dallas Card, and Noah A. Smith</sub><br><sub>*EMNLP 2019*</sub> | <sub><span style="border: 0.5px solid lightgrey; padding: 5px; box-shadow:2px 2px 2px grey; border-radius: 5px; margin-left: 10px; display: inline-block;">[code](https://github.com/allenai/allentune)</span></sub> |
| [Emergent coordination underlying learning to reach to grasp with a brain-machine interface](https://pubmed.ncbi.nlm.nih.gov/29357477)<br><sub> with many authors ðŸ™‚</sub><br><sub>*Journal of Neurophysiology*</sub> | |


#### 2018
-----------

| [Annotation Artifacts in Natural Language Inference Data](https://arxiv.org/abs/1803.02324)<br><sub>**Suchin Gururangan<sup>\*</sup>**, Swabha Swayamdipta<sup>\*</sup>, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith</sub> <br><sub><sup>*</sup>Equal contribution</sub><br><sub>*NAACL 2018*</sub> | |


#### 2014
-----------

| [Analysis of Graph Invariants in Functional Neocortical Circuitry Reveals Generalized Features Common to Three Areas of Sensory Cortex](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003710)<br><sub>**Suchin Gururangan**, Alex Sadovsky and Jason Maclean</sub><br><sub>*Plos Compbio 2014*</sub> | |

